{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNLP-IJCNLP 19 paper #806\n",
    "#### Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels\n",
    "#### Lukas Lange, Michael A. Hedderich, Dietrich Klakow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Bidirectional\n",
    "from keras.layers.merge import concatenate, add\n",
    "\n",
    "from layers import *\n",
    "from ner_datacode import DataCreation, WordEmbedding, WordCluster, LabelRepresentation, Evaluation\n",
    "from experimentalsettings import ExperimentalSettings\n",
    "from noisematrix import NoiseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = ExperimentalSettings.load_json('base')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "\n",
    "The settings are read from the given settings file. The 'config' directory already contains several example configurations. Following options can be set in the file:\n",
    "\n",
    "### Data Settings\n",
    "*   \"NAME\": \"string\"\n",
    "*   \"PATH_TRAIN_CLEAN\": \"str\"    # e.g. data/eng.train\n",
    "*   \"PATH_TRAIN_NOISY\": \"str\"    # e.g. data/eng.autom_labeled.train\n",
    "*   \"PATH_DEV\": \"str\"            # e.g. data/eng.testa\n",
    "*   \"PATH_TEST\": \"str\"           # e.g. data/eng.testb\n",
    "*   \"DATA_SEPARATOR\": \"char\"     # column separator; most often space or tab\n",
    "*   \"WORD_EMBEDDING\": \"str\"      # e.g. data/fasttext/cc.en.300.bin\n",
    "*   \"LABEL_FORMAT\": \"io\"         # either \"io\" or \"bio\"\n",
    "\n",
    "### Model Settings\n",
    "*   \"CONTEXT_LENGTH\": int        # context size of #words to the left and right\n",
    "*   \"LSTM_SIZE\": int             # e.g. 300\n",
    "*   \"DENSE_SIZE\": int            # e.g. 100\n",
    "*   \"DENSE_ACTIVATION\": \"str\"    # e.g. relu\n",
    "*   \"USE_CLEAN\": bool            # either true or false\n",
    "*   \"USE_NOISY\": bool            # either true or false\n",
    "*   \"NOISE_METHOD\": \"str\"        # noise method; either \"channel\",\"cleaning\" or \"dynamic\"\n",
    "*   \"USE_IDENTITY_MATRIX\": bool  # either true or false. This is only used for channel models. \n",
    "*   \"USE_WORD_CLUSTER\": \"str\"    # either \"brown\", \"kmeans\" or \"none\"\n",
    "*   \"PATH_WORD_CLUSTER\": \"str\"   # e.g. data/word_cluster/en_brown_25. This is only used for Brown Clustering. \n",
    "*   \"NUM_WORD_CLUSTER\": int      # e.g. 25. This is only used for kMeans Clustering. \n",
    "*   \"CLEANING_DENSE_SIZE\": int   # e.g. 30. This is only used for Noise Cleaning. \n",
    "\n",
    "### Training Settings\n",
    "*   \"SAMPLE_PCT_CLEAN\": float    # Sample percentage of clean data; e.g. 0.01 for 1%\n",
    "*   \"SAMPLE_PCT_NOISY\": float    # Sample percentage of noisy data; e.g. 0.01 for 1%\n",
    "*   \"EPOCHS\": int                # e.g. 50\n",
    "*   \"BATCH_SIZE\": int            # e.g. 100\n",
    "*   \"WORD_CLUSTER_SELECTION\": float  # e.g. 1.0,\n",
    "*   \"WORD_CLUSTER_INTERPOLATION\": float # e.g. 0.0,\n",
    "*   \"SAMPLE_SEED\": int\n",
    "*   \"TRAINING_SEED\": int\n",
    "\n",
    "# Brown Cluster\n",
    "For our experiments we used the brown clustering implementation from https://github.com/percyliang/brown-cluster\n",
    "Set the PATH_WORD_CLUSTER argument to the resulting path file.\n",
    "The number of clusters has to be specified in the -c argument. \n",
    "The NUM_WORD_CLUSTER setting is only used for kMeans clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of fastText word embeddings\n",
    "word_embedding = WordEmbedding()\n",
    "word_embedding.load_fasttext(SETTINGS[\"WORD_EMBEDDING\"])\n",
    "\n",
    "# LabelRepresentation, either BIO or IO (for testing always BIO)\n",
    "label_representation = LabelRepresentation()\n",
    "if SETTINGS[\"LABEL_FORMAT\"] == \"bio\":\n",
    "    label_representation.use_connl_bio_labels()\n",
    "    test_label_representation = label_representation\n",
    "elif SETTINGS[\"LABEL_FORMAT\"] == \"io\":\n",
    "    label_representation.use_connl_io_labels()\n",
    "    test_label_representation = LabelRepresentation()\n",
    "    test_label_representation.use_connl_bio_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path_to_data, label_representation, remove_label_prefix):\n",
    "    # load dataset\n",
    "    data_creation = DataCreation(input_separator=SETTINGS[\"DATA_SEPARATOR\"])\n",
    "    instances = data_creation.load_connl_dataset(path_to_data, SETTINGS[\"CONTEXT_LENGTH\"], remove_label_prefix)\n",
    "    \n",
    "    # embed words in vector representation\n",
    "    word_embedding.embed_instances(instances)\n",
    "    x = word_embedding.instances_to_vectors(instances)\n",
    "    \n",
    "    # convert BIO/IO labels to one hot vectors\n",
    "    label_representation.embed_instances(instances)\n",
    "    y = label_representation.instances_to_vectors(instances)\n",
    "    \n",
    "    return instances, x, y\n",
    "\n",
    "remove_label_prefix = SETTINGS[\"LABEL_FORMAT\"] == \"io\"\n",
    "train_clean, train_clean_x, train_clean_y = load_and_preprocess_data(SETTINGS[\"PATH_TRAIN_CLEAN\"], \n",
    "                                                                     label_representation, remove_label_prefix)\n",
    "train_noisy, train_noisy_x, train_noisy_y = load_and_preprocess_data(SETTINGS[\"PATH_TRAIN_NOISY\"], label_representation, remove_label_prefix)\n",
    "dev, dev_x, dev_y = load_and_preprocess_data(SETTINGS[\"PATH_DEV\"], test_label_representation, False) # we always test on BIO sheme\n",
    "test, test_x, test_y = load_and_preprocess_data(SETTINGS[\"PATH_TEST\"], test_label_representation, False)\n",
    "\n",
    "del word_embedding.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word clusters\n",
    "if SETTINGS[\"USE_WORD_CLUSTER\"].lower() in ['brown', 'kmeans']:\n",
    "    word_cluster = WordCluster()\n",
    "    if SETTINGS[\"USE_WORD_CLUSTER\"].lower() == 'brown':\n",
    "        word_cluster.load_brown_cluster(SETTINGS[\"PATH_WORD_CLUSTER\"])\n",
    "    elif SETTINGS[\"USE_WORD_CLUSTER\"].lower() == 'kmeans':\n",
    "        emb_values = np.array([x[SETTINGS[\"CONTEXT_LENGTH\"]] for x in train_clean_x])\n",
    "        word_cluster.load_kmeans_cluster(train_clean, emb_values, SETTINGS[\"NUM_WORD_CLUSTER\"])\n",
    "    \n",
    "    def add_cluster_information(instances, word_cluster):\n",
    "        word_cluster.get_cluster(instances)\n",
    "        c = [token.clusterID for token in instances]\n",
    "        return c\n",
    "\n",
    "    train_clean_c = add_cluster_information(train_clean, word_cluster)\n",
    "    train_noisy_c = add_cluster_information(train_noisy, word_cluster)\n",
    "    \n",
    "else:\n",
    "    train_clean_c = [-1 for i in range(len(train_clean))]\n",
    "    train_noisy_c = [-1 for i in range(len(train_noisy))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(xs, ys, cs, size, random_state, sequential):\n",
    "    \"\"\" Creates a subset of the data.\n",
    "    \n",
    "    Args: \n",
    "        xs: Tensor of the input data (x, word embeddings)\n",
    "        ys: Tensor of the labels (vector form)\n",
    "        cs: Tensor of the clusters\n",
    "        size: Size of the subset. Samples at least 1 item if size < 1\n",
    "        random_state: The subset is randomly sampled, instance of np.random_state\n",
    "        sequential: If True, a random start point is picked and then a sequence of instances/words\n",
    "                    is picked. If False, instances are picked randomly.\n",
    "                    \n",
    "    Returns:\n",
    "        subsets of corresponding xs, ys and cs\n",
    "    \n",
    "    \"\"\"\n",
    "    assert len(xs) == len(ys)\n",
    "    assert len(xs) >= size\n",
    "    ind = _get_sample_indicies(len(xs), max(size, 1), random_state, sequential)\n",
    "    xs_sub = np.array([x for i, x in enumerate(xs) if i in ind])\n",
    "    ys_sub = np.array([y for i, y in enumerate(ys) if i in ind])\n",
    "    cs_sub = np.array([y for i, y in enumerate(cs) if i in ind])\n",
    "    return xs_sub, ys_sub, cs_sub\n",
    "\n",
    "def _get_sample_indicies(num_items, num_samples, random_state, sequential):\n",
    "    '''Returns a list of indicies that should be sampled.\n",
    "    \n",
    "    Args:\n",
    "        num_items: integer value representing the pool size\n",
    "        num_samples: integer value representing the number of items to be sampled\n",
    "        random_state: numpy random state that should be used for random processes\n",
    "        sequential: boolean value indicating whether the items should sampled sequentially or completely random\n",
    "        \n",
    "    Returns:\n",
    "        A set of indices\n",
    "    '''\n",
    "    assert num_items >= num_samples\n",
    "    numbers = list(range(num_items))\n",
    "    if num_items == num_samples:\n",
    "        return list(sorted(numbers))\n",
    "    if sequential:\n",
    "        start_number = random_state.randint(0, num_items)\n",
    "        if start_number <= (num_items - num_samples):  # can generate one sequential sample\n",
    "            indicies = numbers[start_number:start_number+num_samples]\n",
    "        else:  # sampled would reach source list bondaries; need to generate two sequential samples\n",
    "            indicies = numbers[start_number:]\n",
    "            indicies.extend(numbers[:num_samples-(num_items-start_number)])\n",
    "    else:\n",
    "        indicies = random_state.randint(0, num_items-1, num_samples)\n",
    "    assert len(indicies) == num_samples\n",
    "    return set(indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_matrix(clean_ys, noisy_ys):\n",
    "    \"\"\"\n",
    "    Computes a noise or confusion matrix between clean and noisy labels.\n",
    "    \n",
    "    Args:\n",
    "        clean_ys: Tensor or list of label vectors of the clean labels\n",
    "        noisy_ys: Tensor or list of label vectors of the noisy labels\n",
    "    \n",
    "    Returns:\n",
    "        A noise matrix of size num_labels x num_labels. Each row represents\n",
    "        p(y_noisy| y_clean=i) for a specific clean label i\n",
    "        (Formula 4 in the paper, without the log)\n",
    "    \"\"\"\n",
    "    num_labels = label_representation.get_num_labels()\n",
    "    assert num_labels == len(clean_ys[0]), f'Expected {num_labels} labels, but got: {len(clean_ys[0])}'\n",
    "    assert len(clean_ys) == len(noisy_ys)\n",
    "    \n",
    "    noise_matrix = np.zeros((num_labels, num_labels))\n",
    "\n",
    "    for clean_y, noisy_y in zip(clean_ys, noisy_ys):\n",
    "        clean_y_idx = np.argmax(clean_y)\n",
    "        noisy_y_idx = np.argmax(noisy_y)\n",
    "                \n",
    "        noise_matrix[clean_y_idx,noisy_y_idx] += 1\n",
    "\n",
    "    for row in noise_matrix:\n",
    "        row_sum = np.sum(row)\n",
    "        if row_sum != 0:\n",
    "            row /= row_sum\n",
    "            \n",
    "    return noise_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Model Architectures\n",
    "\n",
    "Implementations of different noise model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model():\n",
    "    \"\"\"\n",
    "    Creates the base model.\n",
    "    \n",
    "    A Bi-LSTM model with a Dense layer and a Softmax layer for classification.\n",
    "    \"\"\"\n",
    "    input_shape = (SETTINGS[\"CONTEXT_LENGTH\"]*2+1, word_embedding.embedding_vector_size, )\n",
    "\n",
    "    feature_input_layer = Input(shape=input_shape, name=\"input_text\")\n",
    "    bi_lstm_layer = Bidirectional(LSTM(SETTINGS[\"LSTM_SIZE\"]), merge_mode='concat', name=\"bilstm\")(feature_input_layer)\n",
    "    dense_layer = Dense(SETTINGS[\"DENSE_SIZE\"], activation=SETTINGS[\"DENSE_ACTIVATION\"], name=\"dense\")(bi_lstm_layer)\n",
    "    softmax_output_layer = Dense(label_representation.get_num_labels(), activation='softmax', name=\"softmax_out\")(dense_layer)\n",
    "    \n",
    "    model = Model(inputs=[feature_input_layer], outputs=softmax_output_layer)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=\"nadam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model, feature_input_layer, bi_lstm_layer, softmax_output_layer\n",
    "\n",
    "def create_global_noise_layer_model(feature_input_layer, softmax_output_layer, channel_weights):\n",
    "    \"\"\"\n",
    "    Creates a noise-layer model with one global confusion matrix.\n",
    "    \n",
    "    \"Global-CM\" in the paper. This is an implementation of \n",
    "    Hedderich & Klakow: Training a Neural Network in a Low-Resource Setting on Automatically, 2018\n",
    "    \n",
    "    Also used for the feature-dependent noise models (\"Brown-CM\" and \"K-Means-CM\")\n",
    "    \"\"\"\n",
    "    noise_channel = NoiseMatrixLayer(name='noisy-channel', weights=[channel_weights])\n",
    "    channeled_output = noise_channel(softmax_output_layer)\n",
    "    \n",
    "    model = Model(inputs=[feature_input_layer], outputs=channeled_output)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=\"nadam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model, noise_channel\n",
    "\n",
    "def create_noise_cleaning_model(feature_input_layer, bi_lstm_layer):\n",
    "    \"\"\"\n",
    "    Creates a noise-cleaning model that learns how to transform a noisy into a clean label.\n",
    "    \n",
    "    \"Cleaning\" in the paper. This is an implementation of \n",
    "    Veit & al.:  Learning  from  noisy  large-scale  datasets  with  minimal supervision, 2017\n",
    "    adapted to this NER setting.\n",
    "    \"\"\"\n",
    "    noisy_label_input = Input(shape=(label_representation.get_num_labels(),), name=\"noisy_label_input\")\n",
    "    feature_representation_input_to_noise_cleaning = bi_lstm_layer\n",
    "    feature_representation_lower_dimension_dense = Dense(SETTINGS[\"CLEANING_DENSE_SIZE\"], name=\"dense_feature_rep\")(feature_representation_input_to_noise_cleaning)\n",
    "    input_to_noisy_cleaning = concatenate([noisy_label_input, feature_representation_lower_dimension_dense])\n",
    "\n",
    "    dense_noisy_cleaning = Dense(label_representation.get_num_labels(), name=\"dense_noisy_cleaning\")(input_to_noisy_cleaning)\n",
    "    identity_skip = add([dense_noisy_cleaning, noisy_label_input])\n",
    "    clipping = ClipZeroOneLayer()(identity_skip)\n",
    "\n",
    "    noise_cleaning_model = Model(inputs=[feature_input_layer, noisy_label_input], outputs=clipping)\n",
    "    noise_cleaning_model.compile(loss=keras.losses.mean_absolute_error, optimizer=\"nadam\", metrics=['accuracy'])\n",
    "\n",
    "    return noise_cleaning_model\n",
    "\n",
    "def create_dynamic_noise_model():\n",
    "    \"\"\"\n",
    "    Creates a dynamic transition matrix model that creates a transition matrix per input.\n",
    "    \n",
    "    \"Dynamic-CM\" in the paper. This is an implementation of\n",
    "    Luo et al.: Learning with noise:  Enhance distantly supervised relation extraction with \n",
    "                dynamic transition matrix, 2017\n",
    "    adapted to this NER setting.            \n",
    "    \"\"\"\n",
    "    input_shape = (SETTINGS[\"CONTEXT_LENGTH\"]*2+1, word_embedding.embedding_vector_size, )\n",
    "    feature_input_layer = Input(shape=input_shape, name=\"input_text\")\n",
    "    bi_lstm_layer = Bidirectional(LSTM(SETTINGS[\"LSTM_SIZE\"]), merge_mode='concat', name=\"bilstm\")(feature_input_layer)\n",
    "    \n",
    "    # Noise Modeling Part\n",
    "    transition_matrices = DynamicTransitionMatrixGeneration(label_representation.get_num_labels())(bi_lstm_layer)\n",
    "    \n",
    "    #  Prediction Part\n",
    "    dense_layer = Dense(SETTINGS[\"DENSE_SIZE\"], activation=SETTINGS[\"DENSE_ACTIVATION\"], name=\"dense\")(bi_lstm_layer)\n",
    "    ner_output = Dense(label_representation.get_num_labels(), activation='softmax', name=\"softmax_out\")(dense_layer)\n",
    "    \n",
    "    # Combine correct predictions\n",
    "    predict = TransitionMatrixApplication()([transition_matrices, ner_output])\n",
    "    \n",
    "    model = Model(inputs=[feature_input_layer], outputs=[predict]) \n",
    "    # Note that we do not use the trace loss proposed by the authors.\n",
    "    # We found that the standard loss is giving better results in our experiments\n",
    "    # which used fewer training instances than in the original work.\n",
    "    # You can add the trace loss by using sum_loss(cross_entropy, trace_loss). More details\n",
    "    # about sum_loss and trace_loss can be found in layers.py\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=\"nadam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state = np.random.RandomState(SETTINGS[\"TRAINING_SEED\"])\n",
    "\n",
    "def train_epoch(model, data_x, data_y):\n",
    "    \"\"\" Train a single epoch for given X and Y data.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model used for training\n",
    "        data_x: Tensor of embedded words\n",
    "        data_y: Tensor of of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    model.fit(data_x, data_y, batch_size=SETTINGS[\"BATCH_SIZE\"],\n",
    "              epochs=1, verbose=0, shuffle=True)\n",
    "    \n",
    "def train_on_batches(models, data_x, data_y, data_c, random_state=training_state):\n",
    "    \"\"\" Train a single epoch for given X and Y data with the clusters.\n",
    "    \n",
    "    Args:\n",
    "        models: A dictionary of type <clusterID: keras model>\n",
    "        data_x: Tensor of embedded words\n",
    "        data_y: Tensor of one-hot encoded labels\n",
    "        data_c: List of clusterIDs used for splitting the data into batches\n",
    "        random_state: The numpy random state used for shuffling the batches\n",
    "    \"\"\"\n",
    "    batches, tmp_x, tmp_y = [], {}, {}\n",
    "    for c in set(data_c):\n",
    "        tmp_x[c], tmp_y[c] = [], []\n",
    "        \n",
    "    for x, y, c in zip(data_x, data_y, data_c):\n",
    "        if len(tmp_x[c]) == SETTINGS[\"BATCH_SIZE\"]:\n",
    "            batches.append((c, np.array(tmp_x[c]), np.array(tmp_y[c])))\n",
    "            tmp_x[c] = [x]\n",
    "            tmp_y[c] = [y]\n",
    "        else:\n",
    "            tmp_x[c].append(x)\n",
    "            tmp_y[c].append(y)\n",
    "    \n",
    "    # insert incomplete batches\n",
    "    for c in set(data_c):\n",
    "        if len(tmp_x[c]) > 0:\n",
    "            batches.append((c, np.array(tmp_x[c]), np.array(tmp_y[c])))\n",
    "    \n",
    "    random_state.shuffle(batches)\n",
    "    for c, xs, ys in batches:\n",
    "        model = models[c]\n",
    "        model.train_on_batch(xs, ys)\n",
    "\n",
    "def simple_evaluation(model, data, data_x):\n",
    "    evaluation_output = long_evaluation(model, data, data_x)\n",
    "    return Evaluation.extract_f_score(evaluation_output)\n",
    "\n",
    "def long_evaluation(model, data, data_x):\n",
    "    predictions = model.predict(data_x)\n",
    "    predictions = label_representation.predictions_to_labels(predictions)\n",
    "    \n",
    "    # if predictions are in IO format, convert to BIO used for evaluation when working on test set\n",
    "    if SETTINGS[\"LABEL_FORMAT\"] == \"io\":\n",
    "        predictions = LabelRepresentation.convert_io_to_bio_labels(predictions)\n",
    "\n",
    "    evaluation = Evaluation(separator=SETTINGS[\"DATA_SEPARATOR\"])\n",
    "    connl_evaluation_string = evaluation.create_connl_evaluation_format(data, predictions)\n",
    "    return evaluation.evaluate_evaluation_string(connl_evaluation_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    logging.info(\"Create Models\")\n",
    "    \n",
    "    # sample a low-resource setting from the training data\n",
    "    sampling_state = np.random.RandomState(SETTINGS[\"SAMPLE_SEED\"])\n",
    "    clean_x, clean_y, clean_c = create_subset(train_clean_x, train_clean_y, train_clean_c,\n",
    "                                              size=int(len(train_clean_x)*SETTINGS[\"SAMPLE_PCT_CLEAN\"]), \n",
    "                                              random_state=sampling_state, sequential=True)\n",
    "    \n",
    "    # reset to get same instances of noisy data\n",
    "    # pairs of clean and noisy labels are used to initalize the noise model\n",
    "    sampling_state = np.random.RandomState(SETTINGS[\"SAMPLE_SEED\"]) \n",
    "    clean_noisy_x, clean_noisy_y, clean_noisy_c = create_subset(train_noisy_x, train_noisy_y, train_noisy_c,\n",
    "                                              size=int(len(train_noisy_x)*SETTINGS[\"SAMPLE_PCT_CLEAN\"]), \n",
    "                                              random_state=sampling_state, sequential=True)\n",
    "    \n",
    "    \n",
    "    # create model architecture\n",
    "    if SETTINGS[\"NOISE_METHOD\"].lower() == 'dynamic':\n",
    "        base_model = create_dynamic_noise_model()\n",
    "    else:\n",
    "        base_model, feature_input_layer, bi_lstm_layer, softmax_output_layer = create_base_model()\n",
    "        \n",
    "        if SETTINGS[\"NOISE_METHOD\"].lower() == 'channel':\n",
    "            global_matrix = compute_noise_matrix(clean_y, clean_noisy_y)\n",
    "            matrix_figure = NoiseMatrix.visualize_matrix(global_matrix, idx_to_label_name_map=label_representation.label_idx_to_label_name_map)\n",
    "        \n",
    "            # *-Freq functionality in the paper\n",
    "            if SETTINGS[\"WORD_CLUSTER_SELECTION\"] < 1.0:\n",
    "                selection = SETTINGS[\"WORD_CLUSTER_SELECTION\"]\n",
    "                logging.info(f'Selecting {int(len(set(clean_c)) * selection)} noise groups ({selection*100} %)')\n",
    "                freq_groups = Counter(clean_c).most_common(int(len(set(clean_c)) * selection))\n",
    "                freq_groups = set(x[0] for x in freq_groups)\n",
    "                clean_c = np.array([c if c in freq_groups else -1 for c in clean_c])\n",
    "            else:\n",
    "                freq_groups = set(clean_c)\n",
    "            \n",
    "            # *-IP functionality in the paper\n",
    "            if SETTINGS[\"WORD_CLUSTER_INTERPOLATION\"] > 0:\n",
    "                epsilon = SETTINGS[\"WORD_CLUSTER_INTERPOLATION\"]\n",
    "                logging.info(f'Interpolating with epsilon = {epsilon}')\n",
    "            else:\n",
    "                epsilon = 0.0\n",
    "        \n",
    "            channel_models = {} \n",
    "            for clusterID in set(clean_c):\n",
    "                if SETTINGS[\"USE_IDENTITY_MATRIX\"]:\n",
    "                    noise_matrix = np.eye(label_representation.get_num_labels(),\n",
    "                                          label_representation.get_num_labels())\n",
    "                else:\n",
    "                    if clusterID == -1:  # we use all instances for this matrix\n",
    "                        noise_matrix = global_matrix\n",
    "                    else:\n",
    "                        sample = [(y, n) for y, c, n in zip(clean_y, clean_c, clean_noisy_y) if c == clusterID]\n",
    "                        clean_ys, noisy_ys = [i[0] for i in sample], [i[1] for i in sample]\n",
    "                        noise_matrix = compute_noise_matrix(clean_ys, noisy_ys)\n",
    "                    if epsilon > 0:\n",
    "                        noise_matrix = epsilon * global_matrix + (1-epsilon) * noise_matrix\n",
    "         \n",
    "                channel_weights = np.log(noise_matrix + 1e-8)\n",
    "                model, _ = create_global_noise_layer_model(feature_input_layer, softmax_output_layer, channel_weights)\n",
    "                channel_models[clusterID] = model\n",
    "            logging.info(f\"Created 1 Base and {len(channel_models)} Noise Models\")\n",
    "    \n",
    "        elif SETTINGS[\"NOISE_METHOD\"].lower() == 'cleaning':\n",
    "            cleaning_model = create_noise_cleaning_model(feature_input_layer, bi_lstm_layer)\n",
    "            logging.info(f\"Created Noise Cleaning Model\")\n",
    "        \n",
    "    # training loop\n",
    "    best_dev = -1\n",
    "    for epoch in range(SETTINGS[\"EPOCHS\"]):\n",
    "        noisy_x, noisy_y, noisy_c = create_subset(train_noisy_x, train_noisy_y, train_noisy_c,\n",
    "                                                  size=int(len(train_noisy_x)*SETTINGS[\"SAMPLE_PCT_NOISY\"]), \n",
    "                                                  random_state=sampling_state, sequential=False)\n",
    "        \n",
    "        if SETTINGS[\"NOISE_METHOD\"].lower() == 'cleaning':\n",
    "            x_cleaning = {'noisy_label_input': clean_noisy_y,\n",
    "                          'input_text': clean_x}\n",
    "            y_cleaning = clean_y\n",
    "        \n",
    "            # train cleaning component on clean data\n",
    "            cleaning_model.fit(x_cleaning, y_cleaning, batch_size=SETTINGS[\"BATCH_SIZE\"], \n",
    "                               epochs=1, shuffle=True, verbose=0)\n",
    "            \n",
    "            # predict cleaned labels for noisy data\n",
    "            pred_noisy_x = {'noisy_label_input': noisy_y,\n",
    "                            'input_text': noisy_x}\n",
    "            pred_y = cleaning_model.predict(pred_noisy_x)\n",
    "            \n",
    "            cleaned_x = np.concatenate((clean_x, noisy_x), axis=0)\n",
    "            cleaned_y= np.concatenate((clean_y, pred_y), axis=0)\n",
    "            train_epoch(base_model, cleaned_x, cleaned_y)\n",
    "            \n",
    "        elif SETTINGS[\"USE_CLEAN\"]:\n",
    "            train_epoch(base_model, clean_x, clean_y)\n",
    "        \n",
    "        eval_dev = simple_evaluation(base_model, dev, dev_x)\n",
    "        eval_test = simple_evaluation(base_model, test, test_x)\n",
    "        logging.info(f'Epoch {epoch+1}\\tCurrent F1 for DEV: {eval_dev}\\tTEST: {eval_test}')\n",
    "        \n",
    "        # test performance of the model with the best dev performance is used\n",
    "        if eval_dev > best_dev:\n",
    "            best_dev = eval_dev\n",
    "            best_epoch = epoch\n",
    "            test_evaluation = eval_test\n",
    "            long_test_evaluation = long_evaluation(base_model, test, test_x)\n",
    "            \n",
    "        if SETTINGS[\"USE_NOISY\"] and SETTINGS[\"NOISE_METHOD\"].lower() != 'cleaning':\n",
    "            if SETTINGS[\"NOISE_METHOD\"].lower() == 'channel':\n",
    "                noisy_c = np.array([c if c in freq_groups else -1 for c in noisy_c])\n",
    "                train_on_batches(channel_models, noisy_x, noisy_y, noisy_c)\n",
    "            elif SETTINGS[\"NOISE_METHOD\"].lower() in ['none', 'dynamic']:\n",
    "                train_epoch(base_model, noisy_x, noisy_y)\n",
    "\n",
    "    logging.info(long_test_evaluation)\n",
    "    return base_model\n",
    "\n",
    "train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
